{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd90ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import librosa.display\n",
    "import scipy\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fdc087",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4cadebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 16e3 # [Hz] sampling frequency\n",
    "windowLength = 256 # {from paper}\n",
    "fftLength = windowLength\n",
    "overlap = round(0.25 * windowLength) # overlap of 75% makes good prediction; {from paper}\n",
    "window = scipy.signal.hamming(windowLength, sym=False) # Hamming window {from paper}\n",
    "numSegments = 8\n",
    "numFeatures = fftLength // 2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c1bcf",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ccc112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(filepath, sample_rate, normalize=True):\n",
    "    \"\"\"Read an audio file and return it as a numpy array\"\"\"\n",
    "    audio, sr = librosa.load(filepath, sr=sample_rate)\n",
    "    #Normalization?\n",
    "    if normalize:\n",
    "        div_fac = 1 / np.max(np.abs(audio)) / 3.0\n",
    "        audio = audio * div_fac\n",
    "    return audio, sr\n",
    "    \n",
    "def remove_silent_frames(audio):\n",
    "    trimed_audio = []\n",
    "    indices = librosa.effects.split(audio, hop_length = overlap, top_db=20)\n",
    "\n",
    "    for index in indices:\n",
    "        trimed_audio.extend(audio[index[0]: index[1]])\n",
    "    return np.array(trimed_audio)\n",
    "\n",
    "def add_noise_to_clean_audio(clean_audio, noise_signal):\n",
    "    \"\"\"Adds noise to an audio sample\"\"\"\n",
    "    if len(clean_audio) >= len(noise_signal):\n",
    "        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n",
    "        while len(clean_audio) >= len(noise_signal):\n",
    "            noise_signal = np.append(noise_signal, noise_signal)\n",
    "\n",
    "    # Extract a noise segment from a random location in the noise file\n",
    "    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n",
    "    \n",
    "    noise_segment = noise_signal[ind: ind + clean_audio.size]\n",
    "\n",
    "    speech_power = np.sum(clean_audio ** 2)\n",
    "    noise_power = np.sum(noise_segment ** 2)\n",
    "    noisy_audio = clean_audio + np.sqrt(speech_power / noise_power) * noise_segment\n",
    "    return noisy_audio\n",
    "\n",
    "def extend_all_speaches_to_same_size(sample_speach_filenames):\n",
    "    speaches = []\n",
    "    \n",
    "    max_length = 0\n",
    "    for filename in sample_speach_filenames:\n",
    "        sound_audio, sampl_rate_sound = read_audio(filename, sample_rate=fs)\n",
    "        if max_length < len(sound_audio):\n",
    "            max_length = len(sound_audio)\n",
    "        speaches.append([sound_audio, sampl_rate_sound])\n",
    "        \n",
    "    for sound_data in speaches:\n",
    "        sound_data[0] = np.pad(sound_data[0], (0, max_length - len(sound_data[0])), mode='constant')\n",
    "        \n",
    "    return speaches\n",
    "        \n",
    "def play_sound(audio, sample_rate):\n",
    "    ipd.display(ipd.Audio(data=audio, rate=sample_rate)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "104f49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_features(stft_features, numSegments, numFeatures):\n",
    "    noisySTFT = np.concatenate([stft_features[:, 0:numSegments - 1], stft_features], axis=1)\n",
    "    stftSegments = np.zeros((numFeatures, numSegments, noisySTFT.shape[1] - numSegments + 1))\n",
    "\n",
    "    for index in range(noisySTFT.shape[1] - numSegments + 1):\n",
    "        stftSegments[:, :, index] = noisySTFT[:, index:index + numSegments]\n",
    "    return stftSegments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ba139",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "798d9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_noise_dataset = \"../../dataset/noise_dataset/audio/\"\n",
    "all_noise_filenames = glob.glob(os.path.join(path_to_noise_dataset, 'fold*', '*.*'))\n",
    "#print(all_noise_filenames[0:10])\n",
    "\n",
    "path_to_speach_dataset = \"../../dataset/en/clips\"\n",
    "all_speach_filenames = glob.glob(os.path.join(path_to_speach_dataset, '*.*'))\n",
    "#print(all_speach_filenames[0:10])\n",
    "#print(len(all_speach_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6630ed",
   "metadata": {},
   "source": [
    "## Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cd9c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "speach_sample_size = 50 # dataset has ~1e7 sounds, so we need smaller samplegg\n",
    "sample_speach_filenames = random.sample(all_speach_filenames, speach_sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "369d54ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "speach_data_for_nn = []\n",
    "noised_data_for_nn = []\n",
    "\n",
    "for filename in sample_speach_filenames:\n",
    "    sound_audio, sampl_rate_sound = read_audio(filename, sample_rate=fs)\n",
    "    \n",
    "    #play sound_audio without cutting empty voice\n",
    "    #play_sound(sound_audio, sampl_rate_sound)\n",
    "\n",
    "    sound_audio = remove_silent_frames(sound_audio)\n",
    "    \n",
    "    speach_stft = librosa.stft(sound_audio, n_fft=fftLength, win_length=windowLength, hop_length=overlap, window=window, center=True)\n",
    "    speach_stft_abs = np.abs(speach_stft)\n",
    "    speach_stft_phase = np.angle(speach_stft)\n",
    "\n",
    "    #speach_stft_abs_db = librosa.amplitude_to_db(speach_stft_abs,ref=np.max)\n",
    "    #fig, ax = plt.subplots()\n",
    "    #img = librosa.display.specshow(speach_stft_abs_db, y_axis='linear', x_axis='time', ax=ax)\n",
    "    #ax.set_title('Clear speach')\n",
    "    #fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "    \n",
    "    random_noise_signal_index = np.random.randint(0, len(all_noise_filenames) - 1)\n",
    "    noise_audio, sampl_rate_noise = read_audio(all_noise_filenames[random_noise_signal_index], sample_rate=fs)\n",
    "\n",
    "    noise_audio = remove_silent_frames(noise_audio)\n",
    "\n",
    "    noise_stft = librosa.stft(noise_audio, n_fft=fftLength, win_length=windowLength, hop_length=overlap, window=window, center=True)\n",
    "    noise_stft_abs = np.abs(noise_stft)\n",
    "    noise_stft_phase = np.angle(noise_stft)\n",
    "\n",
    "    #noise_stft_abs_db = librosa.amplitude_to_db(noise_stft_abs,ref=np.max)\n",
    "\n",
    "    \n",
    "    noise_and_speach = add_noise_to_clean_audio(sound_audio, noise_audio)    \n",
    "    \n",
    "    noise_and_speach_stft = librosa.stft(noise_and_speach, n_fft=fftLength, win_length=windowLength, hop_length=overlap, window=window, center=True)\n",
    "    noise_and_speach_stft_abs = np.abs(noise_and_speach_stft)\n",
    "\n",
    "    #noise_and_speach_stft_abs_db = librosa.amplitude_to_db(noise_and_speach_stft_abs,ref=np.max)\n",
    "    #fig, ax = plt.subplots()\n",
    "    #img = librosa.display.specshow(noise_and_speach_stft_abs_db, y_axis='linear', x_axis='time', ax=ax)\n",
    "    #ax.set_title('Noisy speach')\n",
    "    #fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "\n",
    "#    speach_data_for_nn.append(speach_stft_abs)\n",
    "#    noised_data_for_nn.append(noise_and_speach_stft_abs)    \n",
    "    \n",
    "    #print(noise_and_speach_stft_abs.shape)\n",
    "    \n",
    "    noise_and_speach_chunks = prepare_input_features(noise_and_speach_stft_abs, numSegments, numFeatures)\n",
    "   \n",
    "    #print(noise_and_speach_chunks.shape, speach_stft_abs.shape)\n",
    "    \n",
    "    noise_and_speach_chunks = np.transpose(noise_and_speach_chunks, (2, 0, 1))\n",
    "    speach_stft_abs = np.transpose(speach_stft_abs, (1, 0))\n",
    "#    noise_stft_phase = np.transpose(noise_stft_phase, (1, 0))\n",
    "\n",
    "    #print(noise_and_speach_chunks.shape, speach_stft_abs.shape)\n",
    "\n",
    "    speach_data_for_nn.append(speach_stft_abs)\n",
    "    noised_data_for_nn.append(noise_and_speach_chunks)\n",
    "    \n",
    "    #play sound_audio\n",
    "    #play_sound(sound_audio, sampl_rate_sound)\n",
    "    #play noise_audio\n",
    "    #play_sound(noise_audio, sampl_rate_noise)\n",
    "    #play noise_and_speach\n",
    "    #play_sound(noise_and_speach, sampl_rate_sound)\n",
    "\n",
    "    #print(noise_and_speach_stft_abs.shape)\n",
    "    #print(\"--\")\n",
    "    \n",
    "#print(\"++\")\n",
    "speach_data_for_nn = np.concatenate(speach_data_for_nn)\n",
    "speach_data_for_nn=speach_data_for_nn[:,:,None]\n",
    "noised_data_for_nn = np.concatenate(noised_data_for_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f1ff154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40838, 129, 8)\n",
      "(40838, 129, 1)\n"
     ]
    }
   ],
   "source": [
    "print(noised_data_for_nn.shape)\n",
    "print(speach_data_for_nn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366df20",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ee04909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79ff805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=TensorDataset(torch.from_numpy(noised_data_for_nn),torch.from_numpy(speach_data_for_nn))\n",
    "\n",
    "train_loader=DataLoader(train_set, batch_size=batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7f900ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for xb,yb in train_loader:\n",
    "    break\n",
    "    for i in range(len(xb)):\n",
    "        noise_and_speach_stft_abs_db = librosa.amplitude_to_db(xb[1],ref=np.max)\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(noise_and_speach_stft_abs_db, y_axis='linear', x_axis='time', ax=ax)\n",
    "        ax.set_title('Noisy speach')\n",
    "        fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "\n",
    "        noise_and_speach_stft_abs_db = librosa.amplitude_to_db(yb[1],ref=np.max)\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(noise_and_speach_stft_abs_db, y_axis='linear', x_axis='time', ax=ax)\n",
    "        ax.set_title('Clear speack')\n",
    "        fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab11a3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38bae560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ad885a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c8f449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Lambda(lambda x: x.view(-1, 1, 129, 8)),\n",
    "    nn.Conv2d(1, 16, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 1, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d((129,1))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4b7d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(a, b):\n",
    "    return ((a - b) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab69acb",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a387652c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoha\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-e3fa0d51e233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m#xb, yb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(\"epoha\")\n",
    "    \n",
    "    for xb,yb in train_loader:\n",
    "        #xb, yb\n",
    "        \n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d7229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
